library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(tidyverse)
library(lubridate)
library(here)
library(vroom)
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
vroom(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <-
list.files(path = here('converted_argos'),
pattern='*.csv',
full.names = T) %>%
map_df(~read_plus(.))
files <- fs::dir_ls(glob = "*csv")
files
files <- fs::dir_ls(glob = "*.csv")
files
files <- fs::dir_ls(here::here('converted_argos'), glob = "*.csv")
files
#> flights_9E.tsv flights_AA.tsv flights_AS.tsv flights_B6.tsv flights_DL.tsv
#> flights_EV.tsv flights_F9.tsv flights_FL.tsv flights_HA.tsv flights_MQ.tsv
#> flights_OO.tsv flights_UA.tsv flights_US.tsv flights_VX.tsv flights_WN.tsv
#> flights_YV.tsv
vroom::vroom(files)
?vroom
#> flights_9E.tsv flights_AA.tsv flights_AS.tsv flights_B6.tsv flights_DL.tsv
#> flights_EV.tsv flights_F9.tsv flights_FL.tsv flights_HA.tsv flights_MQ.tsv
#> flights_OO.tsv flights_UA.tsv flights_US.tsv flights_VX.tsv flights_WN.tsv
#> flights_YV.tsv
map_df(vroom::vroom(files))
mt <- tibble::rownames_to_column(mtcars, "model")
mt
purrr::iwalk(
split(mt, mt$cyl),
~ vroom_write(.x, glue::glue("mtcars_{.y}.csv"), "\t")
)
files <- fs::dir_ls(glob = "mtcars*csv")
files
files <- fs::dir_ls(here::here('converted_argos'), glob = "*.csv")
files
vroom(files, id = "path")
b <- vroom(files, id = "path")
b
locs
vroom_write(locs, str_c(here('output_data'), '/', 'amke_locations', '_', appended_date, '.csv'), delim = ",")
# Read each .csv file with tracking data
tbl <-
dir_ls(path = here::here('converted_argos'),
glob = '*.csv') %>%
map_df(~read_plus(.))
library(fs)
# Read each .csv file with tracking data
tbl <-
dir_ls(path = here::here('converted_argos'),
glob = '*.csv') %>%
map_df(~read_plus(.))
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <-
dir_ls(path = here::here('converted_argos'),
glob = '*.csv') %>%
map_df(~read_plus(.))
tbl
# find the most recent file, which is last in the list due to sorting
output_data_directory <- dir_ls(here::here('output_data'))
output_data_directory
vroom(dir_ls(path = here::here('converted_argos'),
glob = '*.csv'), id = "path")
file <- "https://raw.githubusercontent.com/r-lib/vroom/master/inst/extdata/mtcars.csv"
data <- vroom(file)
data
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
str_c(here::here('output_data'), '/', 'amke_locations', '_', appended_date, '.csv')
library(tidyverse)
library(lubridate)
library(here)
library(vroom)
library(fs)
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <- dir_ls(path = here::here('converted_argos'),
glob = '*.csv') %>%
map_df(~read_plus(.))
tbl
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
# need to figure out a better way to deal with these bad locations:
locs <- locs %>%
filter(Sequence != 52)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
locs
locs <- locs %>%
filter(Date > '2019-01-01' & Longitude > -150 & Longitude < -50)
appended_date <- Sys.Date()
# Write to csv file by date more data was added
vroom_write(locs, str_c(here::here('output_data'), '/', 'amke_locations', '_', appended_date, '.csv'), delim = ",")
library(tidyverse)
library(sf)
library(mapview)
library(fs)
# find the most recent file, which is last in the list due to sorting
output_data_directory <- dir_ls(here::here('output_data'))
last_file_loc <- length(output_data_directory)
# read in file
locs <- read_csv(str_c(here::here('output_data'), '/', output_data_directory[last_file_loc]))
output_data_directory[last_file_loc]
# read in file
locs <- read_csv(output_data_directory[last_file_loc])
# change Argos code to 3 digit ID to make leaflet plotting cleaner
locs <- locs %>%
mutate(TagID = str_sub(TagID, start = 4, end = 6))
# convert to sf object
sf_locs <- sf::st_as_sf(locs, coords = c("Longitude","Latitude")) %>%
sf::st_set_crs(4326)
# create lines
sf_lines <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTILINESTRING")
# create points
sf_points <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
#dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTIPOINT")
# line map
map1 <- sf_lines %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
# point map
map2 <- sf_points %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
# combine together
combinedMap <- map1 + map2
# save as html
mapshot(combinedMap, url = here::here("tracking_map.html"))
last_file_loc
output_data_directory[last_file_loc]
library(fs)
dir_tree(path = "/Users/Jay/Desktop", recurse = TRUE)
dir_tree(path = "/Users/Jay/Desktop", recurse = FALSE)
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
locs
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
locs
locs %>%
filter(Date > '2019-05-01')
locs %>%
filter(Date > '2019-05-01') %>% print(n=Inf)
library(tidyverse)
library(fs)
value <- 'Z:/HeathLab/American Kestrel projects/Full_Cycle_Phenology/2019_nest_box_photos/photos_to_upload_reconyx/NM03/DCIM/100RECNX/IMG_0001.JPG'
str_length(value)
str_sub(value, start = 126, end = 131)
str_sub(value, start = 120, end = 131)
str_sub(value, start = 115, end = 131)
str_sub(value, start = 116, end = 131)
str_sub(value, start = 116, end = 120)
str_sub(value, start = 116, end = 122)
str_sub(value, start = 116, end = 123)
str_sub(value, start = 106, end = 109)
basename(value)
dirname(value)
str_sub(dirname(value), start = -8, end = -1)
str_sub(dirname(value), start = -8, end = -1)
str_sub(dirname(value), start = -18, end = -15)
library(lubridate)
'2019-06-24' + 30
'2019-06-24' + days(30)
as.Date('2019-06-24') + days(30)
locs
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
locs %>%
filter(Date > '2019-01-01')
locs %>%
filter(Date > '2019-06-01')
library(daymetr)
cite(daymetr)
Citation(daymetr)
citation(daymetr)
citation('daymetr')
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(tidyverse)
library(lubridate)
library(here)
library(vroom)
library(fs)
paths <- dir_ls(glob = '*csv')
paths
paths <- dir_ls(here::here('converted_argos'), glob = '*csv')
paths
map_dfr(paths, read.csv, id = 'path')
map_dfr(paths, read_csv, id = 'path')
map(paths, read.csv)
map_dfr(paths, read_csv, id = 'path')
paths <- dir(here::here('converted_argos', pattern = '\\.csv$'))
paths
# Read in the data --------------------------------------------------------
?dir
paths <- dir(here::here('converted_argos', pattern = '\\.csv'))
paths
paths <- dir(here::here('converted_argos', pattern = '*csv'))
paths
paths <- dir(path = here::here('converted_argos', pattern = '\\.csv$'))
paths
map_dfr(paths, read_csv)
paths <- dir_ls(here::here('converted_argos'), glob = '*csv')
map_dfr(paths, read_csv)
?map_dfr
map_dfr(paths, read_csv, .id = 'path')
tbl <- map_dfr(paths, read_csv, .id = 'path')
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(id, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(path, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
tbl
library(vroom)
# Load the libraries ------------------------------------------------------
library(tidyverse)
library(lubridate)
library(here)
library(fs)
library(vroom)
# Read in the data --------------------------------------------------------
# find all the csv files
paths <- dir_ls(here::here('converted_argos'),
glob = '*csv')
# read and combine
tbl <- map_dfr(paths, read_csv, .id = 'path')
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
#tbl %>% print(n=Inf)
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
#locs
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
#locs
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
#locs
# need to figure out a better way to deal with these bad locations:
locs <- locs %>%
filter(Sequence != 52)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
locs
locs <- locs %>%
filter(Date > '2019-01-01' & Longitude > -150 & Longitude < -50)
# Check - how many locations and tags are there?
#table(locs$TagID)
#locs %>% print.data.frame()
appended_date <- Sys.Date()
# Write to csv file by date more data was added
vroom_write(locs, str_c(here::here('output_data'), '/', 'amke_locations', '_', appended_date, '.csv'), delim = ",")
# Load the libraries ------------------------------------------------------
library(tidyverse)
library(lubridate)
library(here)
library(fs)
library(vroom)
# Read in the data --------------------------------------------------------
# find all the csv files
paths <- dir_ls(here::here('converted_argos'),
glob = '*csv')
# read and combine
tbl <- map_dfr(paths, read_csv, .id = 'path')
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(path, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
#tbl %>% print(n=Inf)
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
#locs
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
#locs
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
#locs
# need to figure out a better way to deal with these bad locations:
locs <- locs %>%
filter(Sequence != 52)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
locs
locs <- locs %>%
filter(Date > '2019-01-01' & Longitude > -150 & Longitude < -50)
# Check - how many locations and tags are there?
#table(locs$TagID)
#locs %>% print.data.frame()
appended_date <- Sys.Date()
# Write to csv file by date more data was added
vroom_write(locs, str_c(here::here('output_data'), '/', 'amke_locations', '_', appended_date, '.csv'), delim = ",")
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
locs
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 1) +
theme(legend.position = 'none') +
scale_color_brewer(palette = "Set1")
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_brewer(palette = "Set1")
theme_set(theme_minimal())
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_brewer(palette = "Set1")
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_brewer(palette = "Dark2")
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_brewer(palette = "Set1")
devtools::install_github("johannesbjork/LaCroixColoR")
library(LaCroixColoR)
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
lacroix_palette("Pamplemousse", n = 9, type = "continuous")
?lacroix_palette
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_manual(values = lacroix_palette("PassionFruit", n = 9, type = "discrete"))
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_manual(values = lacroix_palette("PassionFruit", n = 9, type = "continuous"))
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_manual(values = lacroix_palette("Pamplemousse", n = 9, type = "continuous"))
locs %>%
ggplot(., aes(Date, TagID, color = TagID)) +
geom_point(pch = '|', size = 5) +
theme(legend.position = 'none') +
scale_color_manual(values = lacroix_palette("PassionFruit", n = 9, type = "continuous"))
library(here)
library(purrr)
files <- c('combine_csv_files.R', 'mapview.R', 'plot_timeline.R')
map(files, source)
