dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTILINESTRING")
sf_lines
# create points
sf_points <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTIPOINT")
sf_points
# line map
map1 <- sf_lines %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
map1
map2 <- sf_points %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
map2
combinedMap <- map1 + map2
library(htmlwidgets)
combinedMap <- map1 + map2
saveWidget(combinedMap, file=here("tracking_map.html"))
combinedMap
mapshot(combinedMap, file=here("tracking_map.html"))
install_phantomjs()
webshot::install_phantomjs()
mapshot(combinedMap, file=here("tracking_map.html"))
?mapshot
?mapshot
mapshot(combinedMap, url = here("tracking_map.html"))
# find the most recent file, which is last in the list due to sorting
output_data_directory <- list.files(here('output_data'))
last_file_loc <- length(output_data_directory)
# read in file
locs <- read_csv(paste0(here('output_data'), '/', output_data_directory[last_file_loc]))
# change Argos code to 3 digit ID to make leaflet plotting cleaner
locs <- locs %>%
mutate(TagID = str_sub(TagID, start = 4, end = 6))
# convert to sf object
sf_locs <- sf::st_as_sf(locs, coords = c("Longitude","Latitude")) %>%
sf::st_set_crs(4326)
sf_locs
# create lines
sf_lines <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTILINESTRING")
sf_lines
# create points
sf_points <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
#dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTIPOINT")
sf_points
# line map
map1 <- sf_lines %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
map2 <- sf_points %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
# combine together
combinedMap <- map1 + map2
combinedMap
library(tidyverse)
library(sf)
library(mapview)
# find the most recent file, which is last in the list due to sorting
output_data_directory <- list.files(here('output_data'))
last_file_loc <- length(output_data_directory)
# read in file
locs <- read_csv(paste0(here('output_data'), '/', output_data_directory[last_file_loc]))
# change Argos code to 3 digit ID to make leaflet plotting cleaner
locs <- locs %>%
mutate(TagID = str_sub(TagID, start = 4, end = 6))
# convert to sf object
sf_locs <- sf::st_as_sf(locs, coords = c("Longitude","Latitude")) %>%
sf::st_set_crs(4326)
# create lines
sf_lines <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
#dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTILINESTRING")
# create points
sf_points <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
#dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTIPOINT")
# line map
map1 <- sf_lines %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
map2 <- sf_points %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
# combine together
combinedMap <- map1 + map2
# save as html
mapshot(combinedMap, url = here("tracking_map.html"))
library(tidyverse)
library(sf)
library(mapview)
# find the most recent file, which is last in the list due to sorting
output_data_directory <- list.files(here('output_data'))
last_file_loc <- length(output_data_directory)
# read in file
locs <- read_csv(paste0(here('output_data'), '/', output_data_directory[last_file_loc]))
# change Argos code to 3 digit ID to make leaflet plotting cleaner
locs <- locs %>%
mutate(TagID = str_sub(TagID, start = 4, end = 6))
# convert to sf object
sf_locs <- sf::st_as_sf(locs, coords = c("Longitude","Latitude")) %>%
sf::st_set_crs(4326)
# create lines
sf_lines <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
#dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTILINESTRING")
# create lines
sf_lines <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTILINESTRING")
# create points
sf_points <- sf_locs %>%
dplyr::arrange(TagID, Date) %>%
dplyr::group_by(TagID) %>%
#dplyr::summarise(do_union = FALSE) %>%
sf::st_cast("MULTIPOINT")
# line map
map1 <- sf_lines %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
map2 <- sf_points %>%
mapview(
map.types = c("CartoDB.Positron", "Esri.WorldImagery", "Stamen.Terrain", "OpenStreetMap.Mapnik"),
zcol = "TagID", burst = TRUE, legend = FALSE, homebutton = FALSE
)
# combine together
combinedMap <- map1 + map2
# save as html
mapshot(combinedMap, url = here("tracking_map.html"))
library(here)
here <- here::here
source('combine_csv_files.R')
source('mapview.R')
locs
locs %>%
filter(date > '2019-01-01')
locs %>%
filter(Date > '2019-01-01')
locs %>%
filter(Date > '2019-01-01') %>%
tally(TagID)
locs %>%
filter(Date > '2019-01-01') %>%
group_by(TagID)
locs %>%
filter(Date > '2019-01-01') %>%
group_by(TagID) %>%
tally()
library(here)
here <- here::here
source('combine_csv_files.R')
source('mapview.R')
library(here)
here <- here::here
source('combine_csv_files.R')
source('mapview.R')
library(gmailr)
time_in_seconds <- 3*60
Sys.sleep(time_in_seconds) # add delay here to allow time to upload the new map to my github site
me <- "jasonwiniarski@u.boisestate.edu"
PI <- "julieheath@boisestate.edu"
ccList <- c("anjolenehunt@boisestate.edu",
"jessewatson@boisestate.edu",
"fcphenology@boisestate.edu")
## let others know that the new map is ready to view
msg = mime() %>%
from(me) %>%
to(PI) %>%
cc(ccList) %>%
subject(glue("New kestrel tracks - ", Sys.Date())) %>%
text_body("Hi,
Here's an updated map of our tagged kestrels:
https://jaymwin.github.io/tracking_map.html
Jay")
devtools::install_github("tidyverse/glue")
library(tidyverse)
## let others know that the new map is ready to view
msg = mime() %>%
from(me) %>%
to(PI) %>%
cc(ccList) %>%
subject(glue("New kestrel tracks - ", Sys.Date())) %>%
text_body("Hi,
Here's an updated map of our tagged kestrels:
https://jaymwin.github.io/tracking_map.html
Jay")
glue()
glue::glue()
## let others know that the new map is ready to view
msg = mime() %>%
from(me) %>%
to(PI) %>%
cc(ccList) %>%
subject(glue::glue("New kestrel tracks - ", Sys.Date())) %>%
text_body("Hi,
Here's an updated map of our tagged kestrels:
https://jaymwin.github.io/tracking_map.html
Jay")
# create_draft(msg) - to make a draft first
send_message(msg)
msg
glue::glue("New kestrel tracks - ", Sys.Date()))
glue::glue("New kestrel tracks - ", Sys.Date())
paste0("New kestrel tracks - ", Sys.Date())
glue::glue("New kestrel tracks - ", {Sys.Date()})
glue::glue("New kestrel tracks - ", {Sys.Date})
name <- "Fred"
glue('My name is {name}.')
glue::glue('My name is {name}.')
glue::glue("New kestrel tracks - ", as.Date("1991-10-12"))
current_day <- as.Date("1991-10-12")
glue::glue("New kestrel tracks - ", {current_day})
library(tidyverse)
library(lubridate)
library(here)
here::here()
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <-
list.files(path = here('converted_argos'),
pattern='*.csv',
full.names = T) %>%
map_df(~read_plus(.))
tbl
# Use stringr to pull out the tag code from the filename
# Convert date to lubridate (YYYY-MM-DD) format
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
locs
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type)) %>%
select(-type)
locs
?str_c
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
locs
# Load the libraries ------------------------------------------------------
library(tidyverse)
library(lubridate)
library(here)
here::here()
# Read in the data --------------------------------------------------------
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <-
list.files(path = here('converted_argos'),
pattern='*.csv',
full.names = T) %>%
map_df(~read_plus(.))
tbl
# Use stringr to pull out the tag code from the filename
# Convert date to lubridate (YYYY-MM-DD) format
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
tbl %>% print(n=Inf)
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
locs
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
locs
locs %>% print(n=Inf)
# Load the libraries ------------------------------------------------------
library(tidyverse)
library(lubridate)
library(here)
here::here()
# Read in the data --------------------------------------------------------
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <-
list.files(path = here('converted_argos'),
pattern='*.csv',
full.names = T) %>%
map_df(~read_plus(.))
#tbl
# Use stringr to pull out the tag code from the filename
# Convert date to lubridate (YYYY-MM-DD) format
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
#tbl %>% print(n=Inf)
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
#locs
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
#locs
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
#locs
# need to figure out a better way to deal with these bad locations:
locs <- locs %>%
filter(Sequence != 52)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
#locs
# Check - how many locations and tags are there?
#table(locs$TagID)
#locs %>% print.data.frame()
appended_date <- Sys.Date()
# Write to csv file by date more data was added
write_csv(locs, str_c(here('output_data'), '/', 'amke_locations', '_', appended_date, '.csv'))
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(tidyverse)
library(lubridate)
library(here)
here::here()
# Read the file and create a column of the file's name;
# this provides a column to group points by TagID or code
read_plus <- function(flnm) {
read_csv(flnm) %>%
mutate(Filename = flnm)
}
# Read each .csv file with tracking data
tbl <-
list.files(path = here('converted_argos'),
pattern='*.csv',
full.names = T) %>%
map_df(~read_plus(.))
# Use stringr to pull out the tag code from the filename
# Convert date to lubridate (YYYY-MM-DD) format
# Select relevant variables
tbl <- tbl %>%
mutate(TagID = str_sub(Filename, start = -23, end = -18)) %>%
mutate(Date = dmy(Date)) %>%
select(TagID, CRC, Date, Time, Latitude, Longitude, Fix) %>%
arrange(TagID, Date) %>%
distinct()
# Filter out bad points here and duplicates
locs <- tbl %>%
filter(CRC !="Fail") %>%
filter(Fix %in% c("3D", "2D", "A1", "A2", "A3")) %>% # select GPS locations and higher quality Argos location classes
select(-CRC) %>% # this allows you remove duplicates labeled with different CRCs (OK, OK(corrected))
distinct()
# Rename fix type
locs <- locs %>%
mutate(type = case_when(
Fix %in% c("3D", "2D") ~ "GPS",
Fix %in% c('A1', 'A2', 'A3') ~ 'Argos')) %>%
mutate(Fix = str_c(Fix, type, sep = ' ')) %>%
select(-type)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
# need to figure out a better way to deal with these bad locations:
locs <- locs %>%
filter(Sequence != 52)
# Sort it and create fix # by TagID and date
locs <- locs %>%
arrange(TagID, Date) %>%
mutate(Sequence = sequence(rle(.$TagID)$lengths))
locs
locs <- locs %>%
filter(Date > '2019-01-01' & Longitude > -190)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
library(here)
library(purrr)
here <- here::here
#source('combine_csv_files.R')
#source('mapview.R')
#source('email_update.R')
files <- c('combine_csv_files.R', 'mapview.R')
map(files, source)
